# -*- coding: utf-8 -*-
"""pgd_attack

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tS7h3EQkPJ1I0MyQtTYwtcERApLLzUge
"""
import torch
import matplotlib.pyplot as plt
import numpy as np

def projected_gradient_descent(model, x, y, loss_fn, std_params, axi_x, 
                               num_steps, step_size,  alpha, step_norm = "inf", alpha_norm = "inf",
                               clamp=(0,1), device="cuda:0", dataset = "mnist"):
    """Performs the projected gradient descent attack on a batch of images."""
    x_adv = x.clone().detach().requires_grad_(True).to(device)
    x = x.to(device)
    y = y.to(device)
    num_channels = x.shape[1]


    for i in range(num_steps):
        _x_adv = x_adv.clone().detach().requires_grad_(True).to(device)
        
        _x_adv_augmented = torch.cat([_x_adv.to(device), axi_x], dim=0) 
        
        batch_size = x.shape[0]

        
        prediction = model(_x_adv_augmented, std_params)[:batch_size]
        
        y_hat = torch.argmax(prediction, dim = 1)
        loss = loss_fn(prediction, y)
        loss.backward()
        

        
        # saved_var = dict()
        # for tensor_name, tensor in model.named_parameters():
        #     saved_var[tensor_name] = torch.zeros_like(tensor)

        # for j in losses:
        #     j.backward(retain_graph=True)
            
        #     for tensor_name, tensor in model.named_parameters():
        #         new_grad = tensor.grad
        #         saved_var[tensor_name].add_(new_grad)
        #     #optimizer.zero_grad()

        # for tensor_name, tensor in model.named_parameters():
        #     tensor.grad = saved_var[tensor_name] / losses.shape[0]
        
        


        with torch.no_grad():
            # Force the gradient step to be a fixed size in a certain norm
            if step_norm == 'inf':
                gradients = _x_adv.grad.sign() * step_size
            else:
                # Note .view() assumes batched image data as 4D tensor
                gradients = _x_adv.grad * step_size / _x_adv.grad.view(_x_adv.shape[0], -1)\
                    .norm(step_norm, dim=-1)\
                    .view(-1, num_channels, 1, 1)

            x_adv += gradients

        # Project back into l_norm ball and correct range
        if alpha_norm == 'inf':
            # Workaround as PyTorch doesn't have elementwise clip
            x_adv = torch.max(torch.min(x_adv, x + alpha), x - alpha)
        else:
            delta = x_adv - x
            #print(delta)
            # Assume x and x_adv are batched tensors where the first dimension is
            # a batch dimension
            mask = delta.view(delta.shape[0], -1).norm(alpha_norm, dim=1) <= alpha
            print(delta.view(delta.shape[0], -1).norm(alpha_norm, dim=1))
            scaling_factor = delta.view(delta.shape[0], -1).norm(alpha_norm, dim=1)
            scaling_factor[mask] = alpha
            print("scaling:", scaling_factor)

            # .view() assumes batched images as a 4D Tensor
            delta *= alpha / scaling_factor.view(-1, 1, 1, 1)
            print(delta)

            x_adv = x + delta

        
        x_adv = x_adv.clamp(*clamp)
        

       
        

    return x_adv.detach()

def viz_adversarial(x, y_hat = None, dataset="mnist"):
  x = x.clone().cpu().detach().numpy()
  if dataset == "mnist":
    plt.imshow(x[0, 0], cmap='gray')
  elif dataset == "cifar10":
    img = x[2]
    img = img * 0.2 + 0.5
    # img[0] = img[0]*0.229 + 0.485
    # img[1] = img[1]*0.224 + 0.456
    # img[2] = img[2]*0.225 + 0.406
    #print(img)
    plt.imshow(np.transpose(img, (1, 2, 0)))
  
  import datetime
  ts = datetime.datetime.now().timestamp()
  filename = "images/" + str(ts) + ".png"
  plt.savefig(filename)
  
  if y_hat != None: plt.title("predction = {}".format(y_hat[0].item()))

